{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 자동 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "본 글에서는 JAX의 기초적인 자동미분 활용법에 대해서 알아보겠습니다. 마분값을 구하는 것은 현대 머신러닝에서 매우 중요한 부분입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. `jax.grad()`를 통해 기울기 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX에서는 스칼라 값을 리턴하는 함수를 `jax.grad()` 변환함수를 통해 미분할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.070650816\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "grad_tanh = grad(jnp.tanh)\n",
    "print(grad_tanh(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jax.grad()`는 함수를 받고 함수를 내뱉습니다. 만약 파이썬으로 f라는 수학 함수를 구현했다면, `jax.grad(f)`는 $\\nabla f$ 를 내뱉게 됩니다. 즉, `grad(f)(x)` 는 $\\nabla f(x)$ 를 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같이 `jax.grad()`를 여러번 중첩해서 씌울 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.13621868\n",
      "0.25265405\n"
     ]
    }
   ],
   "source": [
    "print(grad(grad(jnp.tanh))(2.0))\n",
    "print(grad(grad(grad(jnp.tanh)))(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax의 자동미분 기능은 고계도함수를 쉽게 구할 수 있도록 해주는데, 함수를 미분한 결과가 곧 또다른 미분 가능한 함수의 형태로 나오기 때문입니다. 고로 고계도함수를 구한다는 건 그저 `jax.grad()` 를 쌓는것만큼 쉽습니다. 아래 변수가 하나인 식을 예로 확인해보겠습니다:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 $f(x) = x^3+2x^2-3x+1$은 아래와 같이 나타낼 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
    "\n",
    "dfdx = jax.grad(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수 `f`의 고계도함수는:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f'(x)=3x^2+4x-3$\n",
    "\n",
    "\n",
    "$f''(x)=6x+4$\n",
    "\n",
    "\n",
    "$f'''(x)=6$\n",
    "\n",
    "\n",
    "$f^{iv}(x)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 `jax.grad()`를 사용하면 파이썬에서도 매우 쉽게 구할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2fdx = jax.grad(dfdx)\n",
    "d3fdx = jax.grad(d2fdx)\n",
    "d4fdx = jax.grad(d3fdx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x=1$일때의 해를 구할 때를 가정하면:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f'(1)=4$\n",
    "\n",
    "$f''(x1)=10$\n",
    "\n",
    "$f'''(1)=6$\n",
    "\n",
    "$f^{iv}(1)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX를 사용했을 떄:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "10.0\n",
      "6.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(dfdx(1.))\n",
    "print(d2fdx(1.))\n",
    "print(d3fdx(1.))\n",
    "print(d4fdx(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 선형회귀 모델의 경사도 계산하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 예제에서는 `jax.grad()`를 통해 선형회기 모델의 경사도를 구하는 방법을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x/2)+1)\n",
    "\n",
    "def predict(W, b, inputs):\n",
    "    return sigmoid(jnp.dot(inputs, W) + b)\n",
    "\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                    [0.88, -1.08, 0.15],\n",
    "                    [0.52, 0.06, -1.30],\n",
    "                    [0.74, -2.49, 1.39]])\n",
    "targets = jnp.array([True, True, False, True])\n",
    "\n",
    "def loss(W, b):\n",
    "    preds = predict(W, b, inputs)\n",
    "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "key, W_key, b_key = jax.random.split(key, 3)\n",
    "W = jax.random.normal(W_key, (3,))\n",
    "b = jax.random.normal(b_key, ())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jax.grad()`를 통해 선형모델의 파라메터 `W`와 `b`에 대한 기울기 값을 구해봅시다. 이때 `jax.grad()`의 `argnums` 인자를 이용하여, 어떤 변수에 대한 기울기를 구하고 싶은지 지정할 수 있습니다. 예를 들어 `argnums=0`일 경우, 첫 번째 인자로 들어간 변수 `W`에 대한 기울기를 출력하는 식입니다. `argnums`의 기본값은 0이므로, `argnums`를 지정하지 않으면 첫 번째 인자에 대한 기울기를 구하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.16965583 -0.8774644  -1.4901346 ]\n",
      "[-0.16965583 -0.8774644  -1.4901346 ]\n",
      "-0.2922724485397339\n",
      "[-0.16965583 -0.8774644  -1.4901346 ]\n",
      "-0.2922724485397339\n"
     ]
    }
   ],
   "source": [
    "W_grad = grad(loss, argnums=0)(W, b)\n",
    "print(f'{W_grad}')\n",
    "\n",
    "W_grad = grad(loss)(W, b)\n",
    "print(f'{W_grad}')\n",
    "\n",
    "b_grad = grad(loss, 1)(W, b)\n",
    "print(f'{b_grad}')\n",
    "\n",
    "W_grad, b_grad = grad(loss, (0, 1))(W, b)\n",
    "print(f'{W_grad}')\n",
    "print(f'{b_grad}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 중첩 리스트, 튜플, 딕셔너리 형태에 대해서 미분하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jax의 강력한 `pytree` 지원 덕에 중첩 리스트, 튜플, 딕셔너리 형태에 대해서도 미분을 할 수 있습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W': Array([-0.16965583, -0.8774644 , -1.4901346 ], dtype=float32), 'b': Array(-0.29227245, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def loss2(params_dict):\n",
    "    preds = predict(params_dict['W'], params_dict['b'], inputs)\n",
    "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
    "    return -jnp.sum(jnp.log(label_probs))\n",
    "\n",
    "print(grad(loss2)({'W': W, 'b': b}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `jax.value_and_grad`를 사용하여 함수의 값과 기울기를 동시에 구하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`jax.value_and_grad()`를 사용하면 함수의 값과 기울기를 동시에 구할 수 있습니다. 연산 또한 값과 기울기를 각각 구하는 것 보다 더욱 효율적입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss value 3.0519388\n",
      "loss value 3.0519388\n"
     ]
    }
   ],
   "source": [
    "loss_value, Wb_grad = jax.value_and_grad(loss, (0, 1))(W, b)\n",
    "print('loss value', loss_value)\n",
    "print('loss value', loss(W, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-jax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
